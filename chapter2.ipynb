{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Coding our First Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2.3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "outputs = (\n",
    "    inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias\n",
    ")\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a single neuron that takes in three inputs and produces a single output, defined by the equation:\n",
    "\n",
    "$$\n",
    "y = w_1 \\cdot x_1 + w_2 \\cdot x_2 + w_3 \\cdot x_3 + b\n",
    "$$\n",
    "\n",
    "where $w_1, w_2, w_3$ are the weights, $x_1, x_2, x_3$ are the inputs, and $b$ is the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A layer of neurons\n",
    "\n",
    "A layer of neurons is simply a group of neurons that take the same input, but produce a different set of outputs. This is because each neuron in a layer (group) has its own set of tunable parameters (weights and biases), which are considered as separate entities from the parameters of the other neurons in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.385</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m4.8\u001b[0m, \u001b[1;36m1.21\u001b[0m, \u001b[1;36m2.385\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights_1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights_2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights_3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias_1 = 2\n",
    "bias_2 = 3\n",
    "bias_3 = 0.5\n",
    "\n",
    "output_1 = (\n",
    "    inputs[0] * weights_1[0]\n",
    "    + inputs[1] * weights_1[1]\n",
    "    + inputs[2] * weights_1[2]\n",
    "    + inputs[3] * weights_1[3]\n",
    "    + bias_1\n",
    ")\n",
    "\n",
    "output_2 = (\n",
    "    inputs[0] * weights_2[0]\n",
    "    + inputs[1] * weights_2[1]\n",
    "    + inputs[2] * weights_2[2]\n",
    "    + inputs[3] * weights_2[3]\n",
    "    + bias_2\n",
    ")\n",
    "\n",
    "output_3 = (\n",
    "    inputs[0] * weights_3[0]\n",
    "    + inputs[1] * weights_3[1]\n",
    "    + inputs[2] * weights_3[2]\n",
    "    + inputs[3] * weights_3[3]\n",
    "    + bias_3\n",
    ")\n",
    "\n",
    "outputs = [output_1, output_2, output_3]\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each neuron in the current layer is connected to all neurons in the previous layer (if any), and each neuron in the previous layer is connected to all neurons in the current layer. This is known as a fully connected layer, or a dense layer.\n",
    "\n",
    "However, this code is not scalable. Instead, we use a loop to handle dynamically-sized inputs and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.21</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.385</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m4.8\u001b[0m, \u001b[1;36m1.21\u001b[0m, \u001b[1;36m2.385\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],  # Neuron 1\n",
    "    [0.5, -0.91, 0.26, -0.5],  # Neuron 2\n",
    "    [-0.26, -0.27, 0.17, 0.87],  # Neuron 3\n",
    "]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "for w_n, b_n in zip(weights, biases):\n",
    "    o_n = 0\n",
    "    for i_i, w_i in zip(inputs, w_n):\n",
    "        o_n += i_i * w_i\n",
    "\n",
    "    o_n += b_n\n",
    "\n",
    "    layer_outputs.append(o_n)\n",
    "\n",
    "print(layer_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the output of a layer of neurons is given by:\n",
    "\n",
    "$$\n",
    "y_n = \\sum_{i=1}^{m} w_{n,i} \\cdot x_i + b_n\n",
    "$$\n",
    "\n",
    "where $m$ is the number of inputs and $n$ is the number of neurons. In the above example, $m=4$ and $n=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors, arrays, and vectors\n",
    "\n",
    "An array is simply a collection of elements, arranged in some logical order. For example, a list of lists of lists can be represented as:\n",
    "```python\n",
    "lll = [[[1, 2, 3, 4], \n",
    "        [5, 6, 7, 8]],\n",
    "       [[9, 10, 11, 12], \n",
    "        [13, 14, 15, 16]],\n",
    "       [[17, 18, 19, 20], \n",
    "        [21, 22, 23, 24]]]\n",
    "```\n",
    "\n",
    "The above is a 3-dimensional array, containing 3 2-dimensional arrays, each containing 2 1-dimensional arrays, each containing 4 elements. In concise notation, the dimensions of the array can be written as `(3, 2, 4)`.\n",
    "\n",
    "That is what a **tensor** is, essentially. From the book, \"a tensor object is an object that can be represented as an array\".\n",
    "\n",
    "Finally, **vectors** are nothing but 1-D arrays, or simple lists in Python, while **matrices** are 2-D arrays, or lists of lists in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product and vector addition\n",
    "\n",
    "A dot product of two vectors is a fancy way of describing the sum of the products of the corresponding elements of the two vectors. The dot product of two vectors $a$ and $b$ is given by:\n",
    "\n",
    "$$\n",
    "a \\cdot b = \\sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \\ldots + a_n b_n\n",
    "$$\n",
    "\n",
    "where $n$ is the number of elements in the vectors. This means that when computing a dot product, both the vectors must have the exact same number of elements.\n",
    "\n",
    "Vector addition is adding the corresponding elements of two vectors to produce a new vector. For example, given two vectors $a$ and $b$, the sum of the two vectors is given by:\n",
    "$$\n",
    "a + b = [a_1 + b_1, a_2 + b_2, \\ldots, a_n + b_n]\n",
    "$$\n",
    "\n",
    "Now, if we tag $a$ as \"inputs$, $b$ as weights, and $c$ as biases, we can write the equation for the output of a single neuron as:\n",
    "$$\n",
    "y = a \\cdot b + c\n",
    "$$,\n",
    "\n",
    "which is essentially the same equation that we computed earlier, but written in a more compact form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a . b:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a . b:  \u001b[1;36m32\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">a + b: \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "a + b: \n",
       "\u001b[1m[\u001b[0m\u001b[1;36m5\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m9\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "\n",
    "dot_product = a[0] * b[0] + a[1] * b[1] + a[2] * b[2]\n",
    "print(\"a . b: \", dot_product)\n",
    "\n",
    "a_plus_b = [a[0] + b[0], a[1] + b[1], a[2] + b[2]]\n",
    "print(\"a + b: \", a_plus_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron math - with NumPy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.8</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m4.8\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A single neuron\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "outputs = np.dot(inputs, weights) + bias\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.8</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.21</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.385</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m4.8\u001b[0m   \u001b[1;36m1.21\u001b[0m  \u001b[1;36m2.385\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A layer of neurons\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],  # Neuron 1\n",
    "    [0.5, -0.91, 0.26, -0.5],  # Neuron 2\n",
    "    [-0.26, -0.27, 0.17, 0.87],  # Neuron 3\n",
    "]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = np.dot(weights, inputs) + biases # Note the order here, which is necessary for the matrix multiplication to work\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A batch of data\n",
    "\n",
    "The input vector that we have been using so far contained 1 single observation, where each value can be thought of as a feature. This makes the entire vector into a **feature set instance**, or more commonly, an **observation** or a **sample**.\n",
    "\n",
    "In practice, we tend to use a bunch of samples instead of just one, because of two reasons:\n",
    "1. It is more efficient to compute the output of a layer of neurons for a bunch of samples at once, rather than one at a time.\n",
    "2. It helps the model to generalize better, as it can learn from a variety of samples, rather than overfitting to a single sample.\n",
    "\n",
    "To perform this in practice, we use the concept of matrix multiplication, ensuring that the dimensions of the matrices are compatible with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.8</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.21</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.385</span> <span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.9</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.81</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>   <span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.41</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.051</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.026</span> <span style=\"font-weight: bold\">]</span>\n",
       " <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.742</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.3957</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2249</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m4.8\u001b[0m     \u001b[1;36m1.21\u001b[0m    \u001b[1;36m2.385\u001b[0m \u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m \u001b[1;36m8.9\u001b[0m    \u001b[1;36m-1.81\u001b[0m    \u001b[1;36m0.2\u001b[0m   \u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m \u001b[1;36m1.41\u001b[0m    \u001b[1;36m1.051\u001b[0m   \u001b[1;36m0.026\u001b[0m \u001b[1m]\u001b[0m\n",
       " \u001b[1m[\u001b[0m \u001b[1;36m0.742\u001b[0m   \u001b[1;36m4.3957\u001b[0m  \u001b[1;36m0.2249\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = [\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "]\n",
    "\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],  # Neuron 1\n",
    "    [0.5, -0.91, 0.26, -0.5],  # Neuron 2\n",
    "    [-0.26, -0.27, 0.17, 0.87],  # Neuron 3\n",
    "]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = (\n",
    "    np.dot(inputs, np.array(weights).T) + biases\n",
    ")  # Note that the weights matrix is transposed here, to obtain a matrix with samples as rows and neurons as columns\n",
    "\n",
    "print(layer_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `layer_outputs` is a 4x3 matrix, where each row corresponds to the output of the neuron layer for a single sample and each column is the output of a single neuron for all samples.\n",
    "\n",
    "Expressing this mathematically, we have:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{1,1} & y_{1,2} & \\dots & y_{1,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y_{m,1} & y_{m,2} & \\dots & y_{m,n}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\dots & x_{1,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{m,1} & x_{m,2} & \\dots & x_{m,p}\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{1,1} & w_{2,1} & \\dots & w_{n,1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{1,p} & w_{2,p} & \\dots & w_{n,p}\n",
    "\\end{bmatrix}^T +\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & \\dots & b_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of samples, $n$ is the number of neurons, and $p$ is the number of features (or inputs to each neuron). In the above example, $m=4$, $n=3$, and $p=4$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
